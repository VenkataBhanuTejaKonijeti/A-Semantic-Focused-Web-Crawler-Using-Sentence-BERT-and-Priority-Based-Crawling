{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDSaUNXM637+LSMluyLXyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkataBhanuTejaKonijeti/A-Semantic-Focused-Web-Crawler-Using-Sentence-BERT-and-Priority-Based-Crawling/blob/main/with_seed_urls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmO-JgZgSfEv"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers beautifulsoup4 requests nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import heapq\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from urllib.parse import urljoin, urlparse\n"
      ],
      "metadata": {
        "id": "vBLtfRAATG0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "STOP_WORDS = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "oUn99xXxTQ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "YrHeZArMTSHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in STOP_WORDS]\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "MbEvaVUcTWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return None, []\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Remove unwanted elements\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=\" \")\n",
        "        text = clean_text(text)\n",
        "\n",
        "        links = []\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            link = urljoin(url, a[\"href\"])\n",
        "            parsed = urlparse(link)\n",
        "            if parsed.scheme in [\"http\", \"https\"]:\n",
        "                links.append(link)\n",
        "\n",
        "        return text, links\n",
        "\n",
        "    except:\n",
        "        return None, []\n"
      ],
      "metadata": {
        "id": "wkBMbjpCTYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_similarity(query_embedding, document_text):\n",
        "    doc_embedding = model.encode(\n",
        "        document_text,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    return float(np.dot(query_embedding, doc_embedding))\n"
      ],
      "metadata": {
        "id": "Jiuz5AoyTa0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_crawler(\n",
        "    seed_urls,\n",
        "    query,\n",
        "    max_pages=20,\n",
        "    relevance_threshold=0.35,\n",
        "    delay=1\n",
        "):\n",
        "    visited = set()\n",
        "    results = []\n",
        "\n",
        "    query_embedding = model.encode(\n",
        "        query,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    # Priority queue (max-heap using negative score)\n",
        "    frontier = []\n",
        "    for url in seed_urls:\n",
        "        heapq.heappush(frontier, (-1.0, url))\n",
        "\n",
        "    while frontier and len(results) < max_pages:\n",
        "        _, url = heapq.heappop(frontier)\n",
        "\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "\n",
        "        print(f\"ðŸ” Fetching LIVE URL: {url}\")\n",
        "\n",
        "        text, links = fetch_page(url)\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        score = semantic_similarity(query_embedding, text)\n",
        "\n",
        "        if score >= relevance_threshold:\n",
        "            results.append({\n",
        "                \"url\": url,\n",
        "                \"score\": round(score, 4)\n",
        "            })\n",
        "\n",
        "            # Add outgoing links with priority\n",
        "            for link in links:\n",
        "                if link not in visited:\n",
        "                    heapq.heappush(frontier, (-score, link))\n",
        "\n",
        "        time.sleep(delay)\n",
        "\n",
        "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)\n"
      ],
      "metadata": {
        "id": "cWWsak43TdPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"covid vaccine pandemic\"\n",
        "\n",
        "SEED_URLS = [\n",
        "    \"https://www.who.int\",\n",
        "    \"https://www.cdc.gov\",\n",
        "    \"https://www.nih.gov\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "8wsN0nCETf_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = semantic_crawler(\n",
        "    seed_urls=SEED_URLS,\n",
        "    query=QUERY,\n",
        "    max_pages=20,\n",
        "    relevance_threshold=0.35,\n",
        "    delay=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "qL2vj_SUTjUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“Œ FINAL RANKED LIVE URL RESULTS\\n\")\n",
        "\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"{i}. Score = {r['score']} | {r['url']}\")\n"
      ],
      "metadata": {
        "id": "PMpDaU-CTsrb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}